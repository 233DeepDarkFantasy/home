{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Unit testing framework\n",
    "\n",
    "## 26.4  unittest — Unit testing framework\n",
    "\n",
    "https://docs.python.org/3/library/unittest.html\n",
    "\n",
    "The unittest unit testing framework was originally inspired by **JUnit** \n",
    "\n",
    "and has a similar flavor as major unit testing frameworks in other languages. \n",
    "\n",
    "It supports test automation, sharing of setup and shutdown code for tests, aggregation of tests into collections, and independence of the tests from the reporting framework.\n",
    "\n",
    "To achieve this, **unittest** supports some important concepts in an object-oriented way:\n",
    "\n",
    "* <b>test fixture</b>: \n",
    "\n",
    "  * A test fixture represents the **preparation** needed to perform one or more tests, and any associate **cleanup actions**. \n",
    "  \n",
    "  This may involve, for example, creating temporary or proxy databases, directories, or starting a server process.\n",
    "\n",
    "\n",
    "* <b>test case</b>: \n",
    "\n",
    "   * A test case is the **individual** unit of testing. \n",
    "   \n",
    "   It checks for a **specific** response to a **particular** set of inputs.\n",
    "   \n",
    "   **unittest** provides a base class,　**TestCase**, which may be used to create new test cases.\n",
    "\n",
    "\n",
    "* <b>test suite</b>: \n",
    "\n",
    "  * A test suite is a **collection** of test cases, test suites, or both.\n",
    "  \n",
    "  It is used to **aggregate tests** that should be executed together.\n",
    "\n",
    "\n",
    "* <b>test runner</b>: \n",
    "\n",
    "  * A test runner is a component which orchestrates the execution of tests and provides the outcome to the user. \n",
    "  \n",
    "  The runner may use a graphical interface, a textual interface, or return a special value to indicate the results of executing the tests\n",
    "\n",
    "\n",
    "## Basic Test Structure\n",
    "\n",
    "Tests, as defined by <b>unittest</b>, have two parts: \n",
    "\n",
    "* <b>code to manage test “fixtures”</b>\n",
    "\n",
    "* <b>the test itself</b>. \n",
    "\n",
    "**Individual tests** are created by \n",
    "\n",
    "* subclassing **TestCase** \n",
    "\n",
    "* overriding or adding appropriate methods \n",
    "\n",
    "For example: \n",
    "\n",
    "* **unittest_simple.py**        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file ./code/unittest/unittest_simple.py\n",
    "\n",
    "import unittest\n",
    "\n",
    "class SimplisticTest(unittest.TestCase):\n",
    "\n",
    "    def test_true(self):\n",
    "        self.assertTrue(True)\n",
    " \n",
    "    def test(self):\n",
    "        self.assertTrue(True)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the <b>SimplisticTest</b> have \n",
    "\n",
    "* <b>test_true()</b> \n",
    "\n",
    "* <b>test()</b>\n",
    "\n",
    "methods would <b>fail if True is ever False</b>.\n",
    "\n",
    "The methods are defined with name \n",
    "\n",
    "* **start** with the letters **test**. \n",
    "\n",
    "This naming convention informs the <b>test runner</b> about which methods represent tests.\n",
    "\n",
    "## Running Tests\n",
    "\n",
    "The easiest way to run unittest tests is to include:\n",
    "```python\n",
    "    if __name__ == '__main__':\n",
    "        unittest.main()\n",
    "```\n",
    "at the bottom of each test file, \n",
    "\n",
    "then simply run the script directly from the **command line**:\n",
    "```    \n",
    "   >python unittest_simple.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./code/unittest/unittest_simple.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./code/unittest/unittest_simple.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "includes \n",
    "\n",
    "* <b>a status indicator for each test</b> \n",
    "\n",
    "   * **”.”** on the first line of output means that a test <b>passed<b>\n",
    "\n",
    "* <b>the amount of time the tests took</b>, \n",
    "\n",
    "\n",
    "For **more** detailed test</b> results, \n",
    "\n",
    "**-v** option:\n",
    "\n",
    "```\n",
    ">python unittest_simple.py -v\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./code/unittest/unittest_simple.py -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run tests with more detailed information by passing in the verbosity argument:\n",
    "```python\n",
    "unittest.main(verbosity=2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file ./code/unittest/unittest_simple_more_detailed_information.py\n",
    "\n",
    "import unittest\n",
    "\n",
    "class SimplisticTest(unittest.TestCase):\n",
    "\n",
    "    def test_true(self):\n",
    "        self.assertTrue(True)\n",
    " \n",
    "    def test(self):\n",
    "        self.assertTrue(True)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(verbosity=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./code/unittest/unittest_simple_more_detailed_information.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Outcomes\n",
    "\n",
    "Tests have 3 possible outcomes:\n",
    "\n",
    "* <b>ok -　　 **.**　　</b>: The test passes \n",
    "\n",
    "* <b>FAIL -　　 **F**　　</b>:The test does not pass, and raises an **AssertionError** exception. \n",
    "\n",
    "* <b>ERROR - 　　**E**　　</b>: The test raises an **exception** other than AssertionError.\n",
    "\n",
    "a test’s status depends on the presence (or absence) of an exception.\n",
    "\n",
    "For Example: \n",
    "\n",
    "* **unittest_outcomes.py**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file ./code/unittest/unittest_outcomes.py\n",
    "\n",
    "import unittest\n",
    "\n",
    "class OutcomesTest(unittest.TestCase):\n",
    "\n",
    "    #   ok\n",
    "    def test_Pass(self):\n",
    "        return\n",
    "\n",
    "    # FAIL\n",
    "    def test_Fail(self):\n",
    "        # AssertionError exception.\n",
    "        self.assertFalse(True)\n",
    "        \n",
    "    # ERROR\n",
    "    def test_Error(self):\n",
    "        # raises an exception other than AssertionError\n",
    "        raise RuntimeError('Test error!')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./code/unittest/unittest_outcomes.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When a test fails or generates an error** \n",
    "\n",
    "the **traceback** is included in the output.\n",
    "In the example above, \n",
    "\n",
    "<b>testFail()</b> fails \n",
    "\n",
    "the traceback <b>shows the line</b> with the failure code.\n",
    "\n",
    "It is up to the person reading the test output to look at the code to figure out the semantic meaning of the failed test, though. \n",
    "\n",
    "#### fail with message\n",
    "\n",
    "To make it <b>easier to understand the nature of a test failure</b>,\n",
    "\n",
    "the <b>fail*() and assert*()</b> methods all accept an argument <b>msg</b>,\n",
    "\n",
    "which can be used to produce <b>a more detailed error message</b>\n",
    "\n",
    "Example: \n",
    "\n",
    "* **unittest_failwithmessage.py**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file ./code/unittest/unittest_failwithmessage.py\n",
    "\n",
    "import unittest\n",
    "\n",
    "class FailureMessageTest(unittest.TestCase):\n",
    "\n",
    "    def test_Fail(self):\n",
    "        self.assertFalse(True,'failure message goes here')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./code/unittest/unittest_failwithmessage.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asserting Truth\n",
    "\n",
    "Most tests assert the truth of some condition. There are a few different ways to write truth-checking tests, depending on the perspective of the test author and the desired outcome of the code being tested. \n",
    "\n",
    "If the code produces a value which can be evaluated as <b>true</b>, the methods <b>assertTrue()</b>  should be used.\n",
    "\n",
    "If the code produces a <b>false</b> value, the methods <b>assertFalse()</b> make more sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file ./code/unittest/unittest_true.py\n",
    "\n",
    "import unittest\n",
    "\n",
    "class TruthTest(unittest.TestCase):\n",
    "\n",
    "    def testAssertTrue(self):\n",
    "        self.assertTrue(True)\n",
    "\n",
    "    def test_AssertFalse(self):\n",
    "        self.assertFalse(False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./code/unittest/unittest_true.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Equality\n",
    "\n",
    "As a special case, `unittest` includes methods for testing <b>the equality of two values</b>.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file ./code/unittest/unittest_equality.py\n",
    "\n",
    "import unittest\n",
    "\n",
    "class EqualityTest(unittest.TestCase):\n",
    "\n",
    "    def test_Equal(self):\n",
    "        self.assertEqual(1, 3)\n",
    "\n",
    "    def test_NotEqual(self):\n",
    "        self.assertNotEqual(2, 3-2)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./code/unittest/unittest_equality.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These special tests are handy, since the values being <b>compared appear in the failure message</b> when a test fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file ./code/unittest/unittest_notequal.py\n",
    "import unittest\n",
    "\n",
    "class InequalityTest(unittest.TestCase):\n",
    "\n",
    "    def test_Equal(self):\n",
    "        self.assertNotEqual(1, 3-2)\n",
    "\n",
    "    def test_NotEqual(self):\n",
    "        self.assertEqual(2, 3-2)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./code/unittest/unittest_notequal.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Almost Equal?\n",
    "\n",
    "In addition to strict equality, it is possible to test for\n",
    "\n",
    "**near equality of floating point numbers** using\n",
    "\n",
    "* assertNotAlmostEqual()\n",
    "\n",
    "* assertAlmostEqual()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file ./code/unittest/unittest_almostequal.py\n",
    "import unittest\n",
    "\n",
    "class AlmostEqualTest(unittest.TestCase):\n",
    "\n",
    "    def test_NotAlmostEqual(self):\n",
    "        self.assertNotAlmostEqual(1.11, 3.3-2.0, places=1)\n",
    "\n",
    "    def test_AlmostEqual(self):\n",
    "        self.assertAlmostEqual(1.1, 3.3-2.0, places=0)\n",
    "\n",
    "    def test_AlmostEqualWithDEfault(self):\n",
    "        self.assertNotAlmostEqual(1.1, 3.3-2.0)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arguments are the values to be compared, and **the number of decimal places** to use for the test.\n",
    "\n",
    "`assertAlmostEquals()` and `assertNotAlmostEqual()`  have an optional parameter named\n",
    "\n",
    "**places** \n",
    "\n",
    "and the numbers are compared by **computing the difference rounded to number of decimal places**.\n",
    "\n",
    "default **places=7**,\n",
    "\n",
    "hence:\n",
    "\n",
    "```python\n",
    "self.assertAlmostEqual(0.5, 0.4) is False \n",
    "```\n",
    "\n",
    "```python\n",
    "self.assertAlmostEqual(0.12345678, 0.12345679) is True.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./code/unittest/unittest_almostequal.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing for Exceptions - assertRaises()\n",
    "\n",
    "If a test **raises** an **exception** `other than` **AssertionError**, it is treated as an **error**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file ./code/unittest/unittest_exception_error.py\n",
    "\n",
    "import unittest\n",
    "\n",
    "class Exception_Error_Test(unittest.TestCase):\n",
    "\n",
    "    # ERROR\n",
    "    def test_Error(self):\n",
    "        # raises an exception other than AssertionError\n",
    "        raise RuntimeError('Test error!')\n",
    "        assertTrue(True)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./code/unittest/unittest_exception_error.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are `circumstances`, however, in which you want the `test` to verify that some code does produce an `exception`.\n",
    "\n",
    "For example, if an `invalid value` is given to an attribute of an object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "password=\"12345\"\n",
    "if (len(password)<6):\n",
    "    raise ValueError('password at least 6 characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "password=\"12345\"\n",
    "\n",
    "try:\n",
    "    if (len(password)<6):\n",
    "        raise ValueError('password at least 6 characters')\n",
    "except ValueError as msg:\n",
    "    print(msg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare these two tests of `invalid value` exception:\n",
    "\n",
    "* trapping the `exception` yourself: `invalid value`\n",
    "\n",
    "* assertRaises()\n",
    "\n",
    "In such cases, \n",
    "\n",
    "```python\n",
    "assertRaises()\n",
    "```\n",
    "makes the code `more clear` than trapping the `exception` yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file ./code/unittest/unittest_exception_pw.py\n",
    "\n",
    "import unittest\n",
    "\n",
    "def raises_pw_error(password):\n",
    "    if (len(password)<6):\n",
    "        raise ValueError('password at least 6 characters')\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "class ExceptionTest(unittest.TestCase):\n",
    "\n",
    "    def test_TrapLocally(self):\n",
    "        try:\n",
    "            raises_pw_error(\"12345\")\n",
    "        except ValueError as msg:\n",
    "            pass\n",
    "        else:\n",
    "            self.fail('Did not see ValueError')\n",
    "\n",
    "    def test_assertRaises(self):\n",
    "        self.assertRaises(ValueError, raises_pw_error,'12345')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./code/unittest/unittest_exception_pw.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file ./code/unittest/unittest_exception.py\n",
    "\n",
    "import unittest\n",
    "\n",
    "def raises_error(*args, **kwds):\n",
    "    print(args, kwds) # *args: tuple; **kwds: dict\n",
    "    if len(args)<2:\n",
    "        raise ValueError('Invalid value: ' + str(args) + str(kwds))\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "class ExceptionTest(unittest.TestCase):\n",
    "\n",
    "    def test_TrapLocally(self):\n",
    "        try:\n",
    "            raises_error('a', b='c') # ('a',), 'b':'c'}\n",
    "            #raises_error('a','b', b='c',d='e') \n",
    "        except ValueError:\n",
    "            pass\n",
    "        else:\n",
    "            self.fail('Did not see ValueError')\n",
    "\n",
    "    def test_assertRaises(self):\n",
    "        self.assertRaises(ValueError, raises_error, 'a', b='c')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./code/unittest/unittest_exception.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The results for both are the `same`, but the second test using `assertRaises()` is more **succinct**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Note\n",
    "\n",
    "```python\n",
    "\n",
    "*args : tuple\n",
    "\n",
    "**kwds: dict\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Fixtures\n",
    "\n",
    "<b>Fixtures are resources needed by a test</b> \n",
    "\n",
    "* if you are writing several tests for the same class, those tests all need **an instance of that class** to use for testing. \n",
    "\n",
    "\n",
    "* test fixtures include `database` connections and temporary `files ` (many people would argue that using external resources makes such tests not “unit” tests, but they are still tests and still useful). \n",
    "\n",
    "** `TestCase` ** includes a special hook to **configure** and **clean up** any fixtures needed by your tests.\n",
    "\n",
    "* To configure the `fixtures`, override `setUp()`.\n",
    "\n",
    "* To clean up, override `tearDown()`.\n",
    "\n",
    "### setUp()\n",
    "\n",
    "Method called to `prepare` the test fixture. This is called immediately `before` calling `the test method`; \n",
    "\n",
    "other than `AssertionError` or `SkipTest`, any `exception` raised by this method will be considered an `error` rather than a test failure. \n",
    "\n",
    "The default implementation does nothing.\n",
    "\n",
    "### tearDown()\n",
    "\n",
    "Method called immediately `after` the test method has been called and the result recorded.\n",
    "\n",
    "This is `called` even if the test method `raised an exception`, so the implementation in subclasses may need to be particularly careful about checking internal state.\n",
    "\n",
    "Any exception, other than `AssertionError` or `SkipTest,` raised by this method will be considered an `error` rather than a test failure. \n",
    "\n",
    "This method will only be called if the `setUp()` succeeds, regardless of the outcome of the test method. \n",
    "\n",
    "The default implementation does nothing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file ./code/unittest/unittest_fixtures.py\n",
    "\n",
    "import unittest\n",
    "\n",
    "class FixturesTest(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        print('In setUp()')\n",
    "        self.fixture = range(1, 10)\n",
    "\n",
    "    def tearDown(self):\n",
    "        print('In tearDown()')\n",
    "        del self.fixture\n",
    "\n",
    "    def test_fixture1(self):\n",
    "        print('in test1()')\n",
    "        self.assertEqual(self.fixture, range(1, 10))\n",
    "     \n",
    "    def test_fixture2(self):\n",
    "        print('in test2()')\n",
    "        self.assertEqual(self.fixture, range(2, 10))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When this sample test is run, you can see \n",
    "\n",
    "* <b>the order of execution</b> of the fixture and test methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./code/unittest/unittest_fixtures.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python doc\n",
    "\n",
    "### 26.4.4. Organizing test code\n",
    "\n",
    "The basic building blocks of unit testing are **test cases**\n",
    "\n",
    "In unittest, test cases are represented by `unittest.TestCase` instances. \n",
    "\n",
    "The simplest TestCase subclass will simply implement a test method\n",
    "\n",
    "* a method whose name starts with **test**\n",
    "\n",
    "**In order** to test something, we use one of the **assert*()** methods provided by the `TestCase` base class.\n",
    "  \n",
    "in order to perform specific testing code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file ./code/unittest/test_DefaultWidgetSizeTestCase.py\n",
    "\n",
    "import unittest\n",
    "\n",
    "class Widget():\n",
    "    def __init__(self, name,size = (40, 40)):\n",
    "        self._name=name\n",
    "        self._size=size\n",
    "\n",
    "    def name(self):\n",
    "        return self._name\n",
    "\n",
    "    def size(self):\n",
    "        return self._size\n",
    "\n",
    "    def resize(self, width, height):\n",
    "        if width < 0  or height < 0:\n",
    "            raise ValueError(\"illegal size\")\n",
    "        self._size = (width, height)\n",
    "\n",
    "    def dispose(self):\n",
    "        pass\n",
    "\n",
    "class DefaultWidgetSizeTestCase(unittest.TestCase):\n",
    "    \n",
    "    def test_default_widget_size1(self):\n",
    "        widget = Widget('The widget1')\n",
    "        \n",
    "        self.assertEqual(widget.size(), (50, 50))\n",
    "     \n",
    "    def test_default_widget_size2(self):\n",
    "        widget = Widget('The widget2')\n",
    "        \n",
    "        self.assertEqual(widget.size(), (40, 40))\n",
    "        \n",
    "    def test_default_widget_size3(self):\n",
    "        widget = Widget('The widget3')\n",
    "        self.assertRaises(ValueError, widget.resize,-1, 10)   \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    unittest.main()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./code/unittest/test_DefaultWidgetSizeTestCase.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests can be numerous, and their set-up can be repetitive. \n",
    "\n",
    "Luckily, we can factor out set-up code by implementing a method called <b>setUp()</b>, which the testing framework will automatically call for **every single test** we run:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "class SimpleWidgetTestCase(unittest.TestCase):\n",
    "   \n",
    "    def setUp(self):\n",
    "        self.widget = Widget('The widget')\n",
    "\n",
    "    def test_default_widget_size(self):\n",
    "        self.assertEqual(self.widget.size(), (50,50),\n",
    "                         'incorrect default size')\n",
    "\n",
    "    def test_widget_resize(self):\n",
    "        self.widget.resize(100,150)\n",
    "        self.assertEqual(self.widget.size(), (100,150),\n",
    "                         'wrong size after resize')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Note:  The order in which the various tests will be run is determined by sorting the test method names with respect to the built-in ordering for strings.\n",
    " \n",
    "\n",
    "If the `setUp()` method raises an exception while the test is running, the framework will consider the test to have suffered an error, and the test method will not be executed.\n",
    "\n",
    "Similarly, we can provide a `tearDown()` method that tidies up after the test method has been run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file ./code/unittest/test_SimpleWidgetTestCase.py\n",
    "\n",
    "import unittest\n",
    "from widget import Widget\n",
    "\n",
    "class SimpleWidgetTestCase(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        self.widget = Widget('The widget')\n",
    "\n",
    "    def tearDown(self):\n",
    "        self.widget.dispose()\n",
    "\n",
    "    def test_default_widget_size(self):\n",
    "        self.assertEqual(self.widget.size(), (50,50),\n",
    "                         'incorrect default size')\n",
    "\n",
    "    def test_widget_resize(self):\n",
    "        self.widget.resize(100,150)\n",
    "        self.assertEqual(self.widget.size(), (100,150),\n",
    "                         'wrong size after resize')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `setUp()` succeeded, `tearDown()` will be run whether the test method succeeded or not.\n",
    "\n",
    "Such a working environment for the testing code is called a `fixture`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./code/unittest/test_SimpleWidgetTestCase.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Suites\n",
    "\n",
    "**Test case instances** are grouped together according to the features they test. \n",
    "\n",
    "`unittest` provides a mechanism for this: the **test suite**, represented by unittest‘s **TestSuite** class. \n",
    "\n",
    "In most cases, calling `unittest.main()` will do the right thing and collect all the module’s test cases for you, and then execute them.\n",
    "\n",
    "However, should you want to `customize` the building of your test suite, you can do it yourself:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file ./code/unittest/test_WidgetTestCase_TestSuite.py\n",
    "\n",
    "import unittest\n",
    "from widget import Widget\n",
    "\n",
    "class WidgetTestCase(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        self.widget = Widget('The widget')\n",
    "\n",
    "    def tearDown(self):\n",
    "        self.widget.dispose()\n",
    "\n",
    "    def test_default_size(self):\n",
    "        self.assertEqual(self.widget.size(), (50,50),\n",
    "                         'incorrect default size')\n",
    "    def test_resize(self):\n",
    "        self.widget.resize(100,150)\n",
    "        self.assertEqual(self.widget.size(), (100,150),\n",
    "                         'wrong size after resize')\n",
    "\n",
    "def suite2():\n",
    "    suite = unittest.TestSuite()\n",
    "    suite.addTest(WidgetTestCase('test_default_size'))\n",
    "    suite.addTest(WidgetTestCase('test_resize'))\n",
    "    return suite\n",
    "\n",
    "def suite1():\n",
    "    suite = unittest.TestSuite()\n",
    "    suite.addTest(WidgetTestCase('test_default_size'))\n",
    "    return suite\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(defaultTest = 'suite2')\n",
    "    #unittest.main(defaultTest = 'suite1')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./code/unittest/test_WidgetTestCase_TestSuite.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUN unittest's main function in Jupyter notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "class SimplisticTest(unittest.TestCase):\n",
    "\n",
    "    def test_true(self):\n",
    "        self.assertTrue(True)\n",
    " \n",
    "    def test(self):\n",
    "        self.assertTrue(True)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**unittest.main** `looks` at **sys.argv** by default, which is what started IPython, hence the error about the kernel connection file not being a valid attribute. \n",
    "\n",
    "You can pass an `explicit list` to main to avoid `looking` up **sys.argv**.\n",
    "\n",
    "In the notebook, you will also want to include `exit=False` to prevent **unittest.main** from trying to shutdown the kernel process:\n",
    "\n",
    "```python\n",
    "unittest.main(argv=['first-arg-is-ignored'], exit=False)\n",
    "```\n",
    "You can pass further arguments in the argv list, e.g.\n",
    "\n",
    "```python\n",
    "unittest.main(argv=['ignored', '-v'], exit=False)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "class SimplisticTest(unittest.TestCase):\n",
    "\n",
    "    def test_true(self):\n",
    "        self.assertTrue(True)\n",
    " \n",
    "    def test(self):\n",
    "        self.assertTrue(True)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['ignored', '-v'], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unittest within Jupyter Notebook -  unittest.TextTestRunner\n",
    "\n",
    "```python\n",
    "unittest.TestLoader().loadTestsFromTestCase()\n",
    "```\n",
    "then,calling **TextTestRunner** with the **stream** parameter will make it work in IPython\n",
    "```python\n",
    "stream=sys.stderr\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(SimplisticTest)\n",
    "\n",
    "#Calling TextTestRunner with the stream parameter will make it work in IPython\n",
    "\n",
    "unittest.TextTestRunner(stream=sys.stderr).run(suite)\n",
    "# unittest.TextTestRunner(verbosity=2,stream=sys.stderr).run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper method for test in jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file unittest_jupyter.py \n",
    "\n",
    "import sys\n",
    "import unittest\n",
    "\n",
    "def run(*test_cases):\n",
    "\n",
    "    for test_case in test_cases:\n",
    "        suite = unittest.TestLoader().loadTestsFromTestCase(test_case)\n",
    "        unittest.TextTestRunner(verbosity=1,stream=sys.stderr).run(suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest_jupyter\n",
    "\n",
    "unittest_jupyter.run(SimplisticTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placing the test code in a `separate` module\n",
    "\n",
    "You can place <b>the definitions of test cases</b> and <b>test suites</b> in the same modules as the code they are to test (such as <b>widget.py</b>),\n",
    "\n",
    "but there are several advantages to placing the `test code` in a **separate** module, such as **test_*widget*.py**:\n",
    "\n",
    "* The test module can be run standalone from the command line.\n",
    "\n",
    "* The test code can more easily be separated from shipped code.\n",
    "\n",
    "* There is less temptation to change test code to fit the code it tests without a good reason.\n",
    "\n",
    "* Test code should be modified much less frequently than the code it tests.\n",
    "\n",
    "* Tested code can be refactored more easily.\n",
    "\n",
    "* Tests for modules written in C must be in separate modules anyway, so why not be consistent?\n",
    "\n",
    "* If the testing strategy changes, there is no need to change the source code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See Also:\n",
    "\n",
    "* 1 unittest ：https://docs.python.org/3/library/unittest.html  The standard library documentation for this module.\n",
    "\n",
    "* 2 nose：https://nose.readthedocs.org/en/latest/   A more sophisticated test manager.\n",
    "\n",
    "* 3 py.test: http://pytest.org/latest   A third-party test runner.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
